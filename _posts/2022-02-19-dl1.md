---
layout: single
title:  "딥러닝 이해하기 with perceptron, regression, Gradient Decent"
categories: DeepLearning
tag: [DeepLearning]
toc: true
toc_sticky: true
tagline: ""
header:
  overlay_image: /assets/images/ai.jpg
  overlay_filter: 0.5

---

### 머신러닝이란?
일반적인 컴퓨터 사이언스는 로직을 직접 구현하는 방식으로 이루어진다. 예를 들어 3과 1이 주어지면 4란 결과값이 나오게 + 란 로직을 구현하는 것이다.
반면에 머신러닝이란 input값인 3과 1 output값인 4 와 같은 많은 개수의 데이터셋을 통해 스스로 학습해 +란 로직을 직접 찾아내는 것이다. 즉, 문제와 답을 주면 규칙을 스스로 찾는 과정이 머신러닝이다.

### 딥러닝이란?
딥러닝이란 머신러닝에 포함되는 개념으로 머신러닝은 사람이 직접 중요한 feature(머신러닝, 딥러닝에서 입력으로 주어지는 변수)를 제공해주고 학습을 진행하는 반면, 딥러닝은 분류에 있어 중요한 feature을 자동적으로 골라내는 작업 즉, 스스로 Feature Extraction을 생성한다.

### 딥러닝이 학습하는것
![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/2.png?raw=true)

### Perceptron으로 딥러닝 이해하기
#### Perceptron의 모형
![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/3.png?raw=true)

Perceptron은 가장 단순한 신경망의 형태로 Hideen Layer가 없이 Single layer 로 형성되어있다.  
입력 feature들과 weights, Activation, 풀력값으로 구성되어있다.

#### perceptron의 구성요소
1. input:  
input은 feature값으로 예를들어 집값 예측의 경우 평수, 역으로부터의 거리, 도시 등등 이 해당된다.  
1. weight:  
input값으로 받은 feature 값들에 얼마만큼의 weight를 주어야하는지 모델이 스스로 weight를 조정해나간다.  
1. weighted sum:  
입력 feature들의 개별 값과 이 개별 feature에 얼마만큼의 가중치를 주어야 하는가를 결정하는 가중치를 각각 곱해서 최종으로 더해서 나온 값  
1. 출력값:  
weighted sum에 Activation 함수를 적용한 값    

#### perceptron의 학습방법 regression(회기) 와 Gradient Descent(경사 하강법)

##### linear regression 으로 이해하는 regression
회귀는 여러 개의 독립변수와 한개의 종속변수 간의 상관관계를 모델링하는 기법이다.  
가령 아파트의 가격을 결정할때 가격을 결정하는 요소에는 방개수, 학군, 지역, 아파트 크기, 지하철역등 무수히 많은 것들을 고려한다.  
이때 아파트는 종속변수이고 나머지 아파트값을 결정하는 요소들은 독립변수이다.  
여기서 회귀란 방개수, 학군, 지역, 아파트 크기, 지하철역등의 얼마만큼의 가중치(회귀 계수)를 부여할지 찾아내는것이다.  
머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회기 계수를 찾아내는 과정이다.
아래의 코드를 통해 실습을 하면서 선형회귀를 통해 회귀를 이용해 보도록 하겠다.


```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

data = pd.read_csv("practice.csv")
x = data["1"]
y = data["1.1"]
plt.plot(x, y, "o")
plt.show()
```


    
![png](output_8_0.png)
    



```python
line_fitter = LinearRegression()
line_fitter.fit(x.values.reshape(-1, 1), y)
plt.plot(x, y, "o")
plt.plot(x, line_fitter.predict(x.values.reshape(-1, 1)))
plt.show()
```


    
![png](output_9_0.png)
    


위의 x,y 데이터를 통해 만들어낸 최적의 선형회귀모델은 그래프에 나와있는 주황색 줄을 의미한다.  
최적의 회귀 모델을 만든다는 것은 바로 전체 데이터의 오류값의 합이 최소가 되는 모델을 만든다는 의미와 동시에 오류 값의 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미이다.  
그렇다면 오류값은 어떻게 측정할까? RSS 와 MSE 가 있다.
 
 - RSS(Residual Sum of Square):
 오류값의 제곱을 구해서 더하는 방식이다. 일반적으로 미분 등의 계산을 편리하게 하기 위해서 RSS 방식으로 올류 합을 구한다.
 - MSE(Mean Squared Error):
 Rssd를 학습 데이터 건수로 나누 값
 
 ![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/4.png?raw=true){: width="60%" height="60%"}
 
 즉 RSS 나 MSE의 값을 좌지우지 하는 것은 오류값에서 실제값과 피처값은 고정되있으므로 회귀계수의 값이다. 즉 독립변수인 <u>회귀계수</u>의 조정을 통해서 오류값을 최소화시키면된다. 


```python
# RSS
predict = line_fitter.predict(x.values.reshape(-1, 1)) #예측값
rss = sum(np.square(y - predict))
mse = rss/len(predict)
print(rss)
print(mse) #최소 오류값
```

    16.437448218724107
    1.8263831354137896
    

##### Gradient Decent(경사 하강법)
위의 선형회귀에서는 두개의 w파라미터를 사용했다. w파라미터의 개수가 적으면 고차원 방정식으로 비용함수가 최소가 되는 w를 구하면되지만. w파라미터의 개수가 무수히 많은 경우 고차원 방정식으로는 해결할 수 없다. 이럴때는 경사하강법을 이용해 해결하면된다.  
경사하강법은 점진적인 반복 계산을 통해 w파라미터의 값을 업데이트하면서 오류값이 최소가 되게하는 w파라미터를 구하는 방식이다. 


```python
from IPython.display import Image
Image(url='https://img.pngio.com/scikit-learn-batch-gradient-descent-versus-stochastic-gradient-descent-png-592_319.png')
```




<img src="https://img.pngio.com/scikit-learn-batch-gradient-descent-versus-stochastic-gradient-descent-png-592_319.png"/>



 - 경사하강법의 과정: 
 비용 함수가 2차선 포물선 형태라고 가정하면 경사 하강법은 최초의 w에서 미분을 적용해 미분값이 계속 감소하는 방향으로 순차적으로 w를 업데이트한다. 그리고 미분값이 증가하지 않을때(예를 들면 이차함수에서 기울기의 값이 0일때) 비용함수를 최소로 하는 w를 반환한다.
 
 - 가중치의 업데이트 과정: 
 ![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/5.png?raw=true){: width="60%" height="60%"}
 ![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/7.png?raw=true){: width="60%" height="60%"}
 
비용 함수인 MSE 는 LOSS 이고 경사하강법은 최적의 w값을 찾는 것이므로 w에 대한 편미분을해 그 값에 학습률을 곱해 새로운 w를 학습의 iteration 수 만큼 계속 갱신해나간다.

##### 경사 하강법 코드실습
sklearn의 diabete자료를 이용하여 코드 실습을 해보겠다.


```python
from sklearn.datasets import load_diabetes
import pandas as pd
import numpy as np

data = load_diabetes()
data.keys()
```




    dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])




```python
dataDF = pd.DataFrame(data.data, columns=data.feature_names)
dataDF["target"] = data.target
dataDF.head()
```





  <div id="df-14f2a870-2d0d-447e-adea-60e210ac25ae">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.038076</td>
      <td>0.050680</td>
      <td>0.061696</td>
      <td>0.021872</td>
      <td>-0.044223</td>
      <td>-0.034821</td>
      <td>-0.043401</td>
      <td>-0.002592</td>
      <td>0.019908</td>
      <td>-0.017646</td>
      <td>151.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.001882</td>
      <td>-0.044642</td>
      <td>-0.051474</td>
      <td>-0.026328</td>
      <td>-0.008449</td>
      <td>-0.019163</td>
      <td>0.074412</td>
      <td>-0.039493</td>
      <td>-0.068330</td>
      <td>-0.092204</td>
      <td>75.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.085299</td>
      <td>0.050680</td>
      <td>0.044451</td>
      <td>-0.005671</td>
      <td>-0.045599</td>
      <td>-0.034194</td>
      <td>-0.032356</td>
      <td>-0.002592</td>
      <td>0.002864</td>
      <td>-0.025930</td>
      <td>141.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.089063</td>
      <td>-0.044642</td>
      <td>-0.011595</td>
      <td>-0.036656</td>
      <td>0.012191</td>
      <td>0.024991</td>
      <td>-0.036038</td>
      <td>0.034309</td>
      <td>0.022692</td>
      <td>-0.009362</td>
      <td>206.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.005383</td>
      <td>-0.044642</td>
      <td>-0.036385</td>
      <td>0.021872</td>
      <td>0.003935</td>
      <td>0.015596</td>
      <td>0.008142</td>
      <td>-0.002592</td>
      <td>-0.031991</td>
      <td>-0.046641</td>
      <td>135.0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-14f2a870-2d0d-447e-adea-60e210ac25ae')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-14f2a870-2d0d-447e-adea-60e210ac25ae button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-14f2a870-2d0d-447e-adea-60e210ac25ae');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




간단하게 age, sex, bmi, bp 이 4가지의 feature을 가지고 target값을 예측해보도록 하겠다.  



```python
def get_update_weights_value(bias, w1, w2, w3, w4, age, sex, bmi, bp, target, learning_rate=0.01): #learning rate는 사람이 정하는것
    N = len(target)
    predict = w1*age + w2*sex + w3*bmi +w4*bp + bias #예측값
    diff = target - predict #실제값 - 예측값
    bias_factors = np.ones((N,)) #초기 bias값

    #weight와 bias를 얼만큼 업데이트 할지 계산
    w1_update = -(2/N)*learning_rate*(np.dot(age.T, diff)) #MSE를 w1에 대해서 미분하고 learning rate를 곱한겂
    w2_update = -(2/N)*learning_rate*(np.dot(sex.T, diff))
    w3_update = -(2/N)*learning_rate*(np.dot(bmi.T, diff))
    w4_update = -(2/N)*learning_rate*(np.dot(bp.T, diff))
    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff))

    #전체적인 error값
    mse_loss = np.mean(np.square(diff)) 

    return bias_update, w1_update, w2_update, w3_update, w4_update, mse_loss

```


```python
def gradient_descent(features, target, iter_epochs=1000, verbose=True):
    w1 = np.zeros((1,))
    w2 = np.zeros((1,))
    w3 = np.zeros((1,))
    w4 = np.zeros((1,))
    bias = np.ones((1, ))

    learning_rate = 0.01
    age = features[:, 0]
    sex = features[:, 1]
    bmi = features[:, 2]
    bp = features[:, 3]

    for i in range(iter_epochs):
        # weight/bias update 값 계산 
        bias_update, w1_update, w2_update, w3_update, w4_update, loss = get_update_weights_value(bias, w1, w2, w3, w4, age, sex, bmi, bp, target, learning_rate=0.01)
        # weight/bias의 update 적용. 
        w1 = w1 - w1_update
        w2 = w2 - w2_update
        w3 = w3 - w3_update
        w4 = w4 - w4_update
        bias = bias - bias_update
        
    return w1, w2 ,w3, w4, bias
   
```


```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(dataDF[['age', 'sex', 'bmi', 'bp']]) #데이터 표준화 코드

w1, w2, w3, w4, bias = gradient_descent(scaled_features, dataDF['target'].values, iter_epochs=5000, verbose=True)
print('##### 최종 w1, w2, w3, w4, bias #######')
print(w1, w2, w3, w4, bias)
```

    ##### 최종 w1, w2, w3, w4, bias #######
    [9.3619626] [-10.3274556] [195.62523273] [104.77434091] [36.53696359]
    

5000번의 perceptron 학습을 통해 학습의 목표인 w1, w2, w3, w4 이 4가지의 weight 값과 bias 값을 구해내었다.  
학습을 통한 최종 perceptron 모습
![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/4.jpg?raw=true){: width="80%" height="80%"}


```python
predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 +scaled_features[:, 2]*w3 + scaled_features[:, 3]*w4 + bias #scaled_features 값은 rm이다. 학습을 scaled된값 따라서 예측도 0에서 1사이의 값으로 해야된다.
dataDF['predicted_result'] = predicted
dataDF.head(10)
```





  <div id="df-10d2c8f7-04ec-48bc-8196-fdaf7fe6d09e">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
      <th>target</th>
      <th>predicted_result</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.038076</td>
      <td>0.050680</td>
      <td>0.061696</td>
      <td>0.021872</td>
      <td>-0.044223</td>
      <td>-0.034821</td>
      <td>-0.043401</td>
      <td>-0.002592</td>
      <td>0.019908</td>
      <td>-0.017646</td>
      <td>151.0</td>
      <td>203.982910</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.001882</td>
      <td>-0.044642</td>
      <td>-0.051474</td>
      <td>-0.026328</td>
      <td>-0.008449</td>
      <td>-0.019163</td>
      <td>0.074412</td>
      <td>-0.039493</td>
      <td>-0.068330</td>
      <td>-0.092204</td>
      <td>75.0</td>
      <td>107.055560</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.085299</td>
      <td>0.050680</td>
      <td>0.044451</td>
      <td>-0.005671</td>
      <td>-0.045599</td>
      <td>-0.034194</td>
      <td>-0.032356</td>
      <td>-0.002592</td>
      <td>0.002864</td>
      <td>-0.025930</td>
      <td>141.0</td>
      <td>181.271876</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.089063</td>
      <td>-0.044642</td>
      <td>-0.011595</td>
      <td>-0.036656</td>
      <td>0.012191</td>
      <td>0.024991</td>
      <td>-0.036038</td>
      <td>0.034309</td>
      <td>0.022692</td>
      <td>-0.009362</td>
      <td>206.0</td>
      <td>128.793333</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.005383</td>
      <td>-0.044642</td>
      <td>-0.036385</td>
      <td>0.021872</td>
      <td>0.003935</td>
      <td>0.015596</td>
      <td>0.008142</td>
      <td>-0.002592</td>
      <td>-0.031991</td>
      <td>-0.046641</td>
      <td>135.0</td>
      <td>139.344517</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.092695</td>
      <td>-0.044642</td>
      <td>-0.040696</td>
      <td>-0.019442</td>
      <td>-0.068991</td>
      <td>-0.079288</td>
      <td>0.041277</td>
      <td>-0.076395</td>
      <td>-0.041180</td>
      <td>-0.096346</td>
      <td>97.0</td>
      <td>114.189819</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.045472</td>
      <td>0.050680</td>
      <td>-0.047163</td>
      <td>-0.015999</td>
      <td>-0.040096</td>
      <td>-0.024800</td>
      <td>0.000779</td>
      <td>-0.039493</td>
      <td>-0.062913</td>
      <td>-0.038357</td>
      <td>138.0</td>
      <td>102.516272</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.063504</td>
      <td>0.050680</td>
      <td>-0.001895</td>
      <td>0.066630</td>
      <td>0.090620</td>
      <td>0.108914</td>
      <td>0.022869</td>
      <td>0.017703</td>
      <td>-0.035817</td>
      <td>0.003064</td>
      <td>63.0</td>
      <td>176.565418</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.041708</td>
      <td>0.050680</td>
      <td>0.061696</td>
      <td>-0.040099</td>
      <td>-0.013953</td>
      <td>0.006202</td>
      <td>-0.028674</td>
      <td>-0.002592</td>
      <td>-0.014956</td>
      <td>0.011349</td>
      <td>110.0</td>
      <td>177.576434</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.070900</td>
      <td>-0.044642</td>
      <td>0.039062</td>
      <td>-0.033214</td>
      <td>-0.012577</td>
      <td>-0.034508</td>
      <td>-0.024993</td>
      <td>-0.002592</td>
      <td>0.067736</td>
      <td>-0.013504</td>
      <td>310.0</td>
      <td>169.042522</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-10d2c8f7-04ec-48bc-8196-fdaf7fe6d09e')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-10d2c8f7-04ec-48bc-8196-fdaf7fe6d09e button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-10d2c8f7-04ec-48bc-8196-fdaf7fe6d09e');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




target값과 predict_result의 값을 비교해보면 차이가 어느정도 나는것을 볼 수 있는데 이는 10가지의 feature 중 4가지의 feature만 사용해 차이가 나는 것 같다.
