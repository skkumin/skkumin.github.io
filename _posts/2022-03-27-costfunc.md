---
layout: single
title:  "Neural Network 손실함수(Loss funstion, Cost function) "
categories: DeepLearning
tag: [DeepLearning, loss function]
toc: true
toc_sticky: true
tagline: ""
header:
  overlay_image: /assets/images/ai.jpg
  overlay_filter: 0.5
use_math: true
---

### 손실 함수란?
 - 신경망은 "하나의 지표"를 기준으로 최적의 매개변수 값을 탐색한다. 신경망 학습에서 사용하는 지표를 손실함수라고 한다.
 - 손실 함수는 신경망 성능의 "나쁨"을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 "못"하느냐를 나타낸다.
 - 일반적으로 손실함수로 (교차 엔트로피) 와 (오차제곱합)을 사용한다.

### 왜 손실함수를 이용하는가?
"정확도"라는 지표를 나두고 "손실함수"라는 우회적인 방법을 선택하는 이유는 바로 신경망 학습에서의 "미분"때문이다. 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능환 한 작게 하는 매개변수 값을 찾는다. 이때 매개변수의 미분을 계산하고, 그 미분값을 단서로 매개 변수의 값을 서서히 갱신하는 과정을 반복한다.  
미분값이 음수면 가중치 매개변수를 양의 방향으로 변환시켜 손실 함수의 값을 줄일 수 있고, 반대로 미분 값이 양수이면 가중치 매개변수를 음의 방향으로 변환시켜 손실 함수의 값을 줄일 수 있다.

### MSE(Mean Squared Error)
 - regression문제의 손실함수로 사용<br/>
 <br/>
  $$\frac{1}{N}\sum_{i=1}^{N} (y_i- \hat{y_i})^2$$<br/>
  $$ \hat{y_i}: 신경망의 출력(신경망이 추정한 값),   y_i: 실제값,   N: 데이터의 차원의 수 $$<br/>


```python
def mse(y, y_i):
    error = np.sum(np.square(y - y_i))/2
    
    return error
```


```python
y = np.array([1, 3, 5, 6, 7])
y_i = np.array([3, 2, 7, 5, 8])

mse(y, y_i)
```




    5.5



### Softmax에 해당하는 Cross Entropy Error(CEE)
 - classification의 문제중 마지막층이 Softmax인 즉, multiclass인 경우의 손실함수<br/>  
 $$-\frac{1}{N}\sum_{i=1}^{N} y_i*\log{(\hat{y_i})}$$<br/>
 $$ N: 데이터수, y_i:실제값, \hat{y_i}:예측값$$ <br/>
 <br/>
![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/softmaxentropy.png?raw=true){: width="40%" height="40%"}<br/>
<br/>
 - 위는 한건의 데이터에 대한 multi classification의 예이며 이에대한 CEE는 다음과 같다.<br/>  
 $$CEE = \sum_{i=1}^{class의 개수} i번째 클래스 실제값*\log{(i번째 클래스 예측값)}$$
 $$  = -\log{(2번째예측값)} =  -\log{(0.9)} = 0.105$$<br/>

 - 즉 멀티클래스에서 CEE값은 다음과 같다.<br/>  
 $$CEE = -\frac{1}{N}\sum_{i=1}^{N}-\log{(정답label에 해당하는 softmax 예측값)}$$
 $$ N: 데이터수$$ 

softmax cross entropy 함수 생성


```python
def softmax_cee(y, y_i):
    result = 0
    for i in range(len(y)):
        result += -np.log(np.sum(y_i[i]*y[i]))
        print(f"{i+1}번째 데이터에 대한 CEE={-np.log(np.sum(y_i[i]*y[i]))}")
    return (result/len(y))    
```


```python
y_1 = [0.3, 0.1, 0.6]
y_2 = [0.3, 0.5, 0.2]
y_3 = [0.4, 0.1, 0.5]
y_i = np.array([y_1, y_2, y_3])

y1 = [1, 0, 0]
y2 = [0, 1, 0]
y3 = [0, 0, 1]
y = np.array([y1, y2, y3])

softmax_cee(y, y_i)
```

    1번째 데이터에 대한 CEE=1.2039728043259361
    2번째 데이터에 대한 CEE=0.6931471805599453
    3번째 데이터에 대한 CEE=0.6931471805599453
    




    0.8634223884819422




```python
y_1 = [0.6, 0.1, 0.3]
y_2 = [0.3, 0.5, 0.2]
y_3 = [0.4, 0.1, 0.5]
y_i = np.array([y_1, y_2, y_3])

y1 = [1, 0, 0]
y2 = [0, 1, 0]
y3 = [0, 0, 1]
y = np.array([y1, y2, y3])

softmax_cee(y, y_i)
```

    1번째 데이터에 대한 CEE=0.5108256237659907
    2번째 데이터에 대한 CEE=0.6931471805599453
    3번째 데이터에 대한 CEE=0.6931471805599453
    




    0.6323733282952938



>위의 코드를 보면 두 셀의 y는 동일하고 y_i만 다른걸 볼수 있다. 아래의 y_i값이 더 잘 예측된 값인데 `softmax_cee`로 구해본 결과 더 잘 예측된 값인 아래의 셀의 cee가 더 낮게 나온것을 볼 수 있다.

### Sigmoid에 해당하는 Cross Entropy Error(CEE)
 - classification의 문제중 마지막층이 Sigmoid인 즉, 이진분류(binary)인 경우의 손실함수
 - classification문제의 손실 함수로 사용<br/>   
 $$-\frac{1}{N}\sum_{i=1}^{N} y_i*\log{(\hat{y_i})}+(1-y_i)*\log{(1-\hat{y_i})}$$<br/>
 $$ N: 데이터수, y_i:실제값, \hat{y_i}:예측값$$ <br/>
 - 식을 보게 되면 이진분류에서 실제값인 y_i는 1아니면 0의값이므로 실제값이 0일경우는 앞의 식이 0이되고 실제값이 1인 경우에는 뒤의 식이 0이 된다.


```python
def sigmoid_cee(y, y_i):
    result = 0
    for i in range(len(y)):
        result += -(y[i]*np.log(y_i[i]) + (1-y[i])*np.log(1-y_i[i]))
        print(f"{i+1}번째 데이터에 대한 CEE={-(y[i]*np.log(y_i[i]) + (1-y[i])*np.log(1-y_i[i]))}")
    return result/len(y) 
```


```python
y_i = [0.5, 0.7, 0.3, 0.2, 0.7, 0.9]
y = [0, 1, 0, 0, 1, 1]
sigmoid_cee(y, y_i)
```

    1번째 데이터에 대한 CEE=0.6931471805599453
    2번째 데이터에 대한 CEE=0.35667494393873245
    3번째 데이터에 대한 CEE=0.35667494393873245
    4번째 데이터에 대한 CEE=0.2231435513142097
    5번째 데이터에 대한 CEE=0.35667494393873245
    6번째 데이터에 대한 CEE=0.10536051565782628
    




    0.34861267989136313



>이론상 위의 데이터중 에측값이 0.9이고 label값이 1인 데이터의 loss값이 제일 작아야하는데 구현한 `sigmoid_cee`함수 결과값을 보면 이에 해당하는 6번째 데이터에 대한 CEE값이 제일 작은것을 확인할 수 있다.
