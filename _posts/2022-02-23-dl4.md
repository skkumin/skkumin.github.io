---
layout: single
title:  "Neural Network 활성화 함수(Activation Function) "
categories: DeepLearning
tag: [DeepLearning, Activation Function]
toc: true
toc_sticky: true
tagline: ""
header:
  overlay_image: /assets/images/ai.jpg
  overlay_filter: 0.5

---


### 활성화함수

 - 활성화 함수의 사용 이유:  
   - 활성화 함수는 이전 층의 output값을 함수에 적용시켜 다음 층의 뉴런으로 신호를 전달한다.  
   - 활성화 함수가 필요한 이유는 모델의 복잡도를 올리기 위함인데 모델의 복잡도를 올리는 이유는 비선형 문제를 해결하기 위해서이다.  
   - perceptron만으로는 비선형 문제를 해결할 수 없어 Nueral Network 방식을 사용하지만 hidden layer을 만든다고 해서 비선형 문제를 해결할 수 있는것이 아니다.  
   - 활성 함수를 사용하면 입려값에 대한 출력값이 비선형적으로 나오게된다.  
 

### 대표적인 활성화 함수


```python
import numpy as np
import matplotlib.pyplot as plt
```

#### Sigmoid

 - 이진 분류(적합/부적합, yes/no...)시 마지막 classification에 사용된다.  
 - 0또는 1값을 반환하는 시그모이드의 특성으로 인해 이진 분류의 확률값을 기반으로 최종 분류 예측을 적용하는데 시그모이드가 사용된다.  
 - Vanishing Gradient 등의 이슈로 은닉층에서의 활성화함수로는 더 이상은 사용되지 않는다.  
 - 이진분류에서 예측 값이 0.5 이상이면 1로, 0.5 이하는 0으로 분류될 것이다.

Vanishing Gradient:  
층이 많아 질수록 오차역전파 수행시 미분값이 0에 다다르게 되어서 update해야되는 Gradient가 소실되어 가중치가 갱신이 되지 않아 학습이 중단되게 되는것


```python
def sigmoid(x):
    
    return 1/(1+np.exp(-x))
```

sigmoid함수
![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/sig_real.png?raw=true)
sigmoid미분 함수
![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/diff_sigmoid.png?raw=true)

그래프의 미분계수를 보면 최댓값은 0.25이다. backpropagation과정에 화성화함수의 미분값을 곱하는 과정이 있기 때문에 0.25의 매우작은 값으로 인해 vanishing gradient의 문제가 생기는 것이다.

#### ReLU
 - 대표적인 은닉층의 활성화함수
 - 입력값이 0보다 작을때는 0을, 입력값이 0보다 클때는 입력값을 출력
 - 편미분 시 기울기가 1로 일정하므로, 가중치 업데이트 속도가 매우 빠르다.  
 - Vanishing Gradient의 문제가 발생하지 않는다.
 - 가중치가 업데이트 되는 과정에서 가중치 합이 음수가 되는 순간 ReLU는 0을 반환하기 때문에 해당 뉴런은 그 이후로 0만 반환하는 죽은 뉴런 현상이 생길 수 있다.


```python
def relu(x):
    
    return np.maximum(0, x)
```

ReLU함수 그래프
![](https://github.com/skkumin/skkumin.github.io/blob/master/images/deeplearning/relu.png?raw=true)

#### Softmax
 - 멀티 분류시(고양이, 강아지, 사람 등등판별하기) 시 마지막 classification에서 사용
 - sigmoid와 유사하게 Score값을 확률값 0 ~ 1 로 변환 하지만, 차이점은 소프트 맥스는 개별 출력값의 총 합이 1이 되도록 매핑해 준다.


```python
def softmax(x):
    exp_x = np.exp(x)
    sum_exp = np.sum(exp_x)
    y = exp_x/sum_exp
    
    return y
```


```python
import pandas as pd

x = np.array([2, 10, 1, 4])
y = softmax(x)

df = pd.DataFrame({"sotfmax 전 값": x, "sofmax 값": y,
                   "멀티분류": ["개", "고양이", "강아지", "사람"]})
df.set_index("멀티분류")
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sotfmax 전 값</th>
      <th>sofmax 값</th>
    </tr>
    <tr>
      <th>멀티분류</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>개</th>
      <td>2</td>
      <td>0.000334</td>
    </tr>
    <tr>
      <th>고양이</th>
      <td>10</td>
      <td>0.997071</td>
    </tr>
    <tr>
      <th>강아지</th>
      <td>1</td>
      <td>0.000123</td>
    </tr>
    <tr>
      <th>사람</th>
      <td>4</td>
      <td>0.002471</td>
    </tr>
  </tbody>
</table>
</div>



위의 표를 보게되면 이모델로 평가된 데이터는 softmax값들 중 제일큰 고양이이다.
